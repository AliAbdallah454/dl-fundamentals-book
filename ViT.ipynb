{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58977e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efe1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 144\n",
    "PATCH_SIZE = 8\n",
    "EMB_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945ffe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792M/792M [00:36<00:00, 21.8MB/s] \n",
      "100%|██████████| 19.2M/19.2M [00:02<00:00, 9.35MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((144, 144)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(images, num_samples=20, cols=4):\n",
    "    \"\"\"Plots some samples from the dataset\"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    idx = int(len(dataset) / num_samples)\n",
    "    print(images)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        if i % idx == 0:\n",
    "            plt.subplot(int(num_samples / cols) + 1, cols, int(i / idx) + 1)\n",
    "            plt.imshow(to_pil_image(img[0]))\n",
    "\n",
    "\n",
    "dataset = torchvision.datasets.OxfordIIITPet('./ds', download=True, transform=transform)\n",
    "# show_images(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932ec5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa82b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 144, 144]), torch.Size([1, 324, 128]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chans=3, patch_size=8, emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.unfold = nn.Unfold(patch_size, stride=patch_size)\n",
    "        self.proj = nn.Linear(3 * patch_size * patch_size, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.unfold(x) # (B, C*P*P, N)\n",
    "        x = x.transpose(-1, -2) # (B, N, CPP)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "        \n",
    "inp = torch.randn(1, 3, 144, 144)\n",
    "pe = PatchEmbedding(in_chans=3, patch_size=8, emb_dim=128)\n",
    "inp.shape, pe(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc72a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim=32, n_heads=2, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(emb_dim, n_heads, dropout, batch_first=True)\n",
    "\n",
    "        self.q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.k = nn.Linear(emb_dim, emb_dim)\n",
    "        self.v = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        attn_output, attn_output_weights = self.attention(q, k, v)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a902753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attention()(torch.zeros(1, 5, 32))\n",
    "attn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c1a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prenorm(nn.Module):\n",
    "\n",
    "    def __init__(self, fn, emb_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fn(self.layer_norm(x)) # Attn(LN(x))\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47eb5e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4])\n",
      "tensor([[[ 1.,  1.,  1.,  1.],\n",
      "         [ 1.,  2.,  3., 10.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.8485, -0.5657, -0.2828,  1.6971]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = torch.tensor([\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 2, 3, 10]\n",
    "], device='cpu', dtype=torch.float32)\n",
    "\n",
    "last_dim = att.shape[-1]\n",
    "att = torch.unsqueeze(att, 0)\n",
    "layer_norm = nn.LayerNorm(last_dim, device='cpu')\n",
    "\n",
    "print(att.shape)\n",
    "print(att)\n",
    "layer_norm(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1b6b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0205,  0.0404,  0.0117, -0.0022,  0.0241, -0.0385, -0.0441,\n",
       "           0.0777,  0.0837, -0.0639, -0.0931, -0.0232, -0.0108, -0.0322,\n",
       "           0.0176,  0.0527,  0.0941, -0.0132,  0.0397, -0.0193, -0.0001,\n",
       "           0.0213, -0.0204,  0.0206, -0.0676, -0.0797, -0.0829, -0.0929,\n",
       "           0.0083,  0.0649,  0.0093, -0.0020],\n",
       "         [-0.0205,  0.0404,  0.0117, -0.0022,  0.0241, -0.0385, -0.0441,\n",
       "           0.0777,  0.0837, -0.0639, -0.0931, -0.0232, -0.0108, -0.0322,\n",
       "           0.0176,  0.0527,  0.0941, -0.0132,  0.0397, -0.0193, -0.0001,\n",
       "           0.0213, -0.0204,  0.0206, -0.0676, -0.0797, -0.0829, -0.0929,\n",
       "           0.0083,  0.0649,  0.0093, -0.0020],\n",
       "         [-0.0205,  0.0404,  0.0117, -0.0022,  0.0241, -0.0385, -0.0441,\n",
       "           0.0777,  0.0837, -0.0639, -0.0931, -0.0232, -0.0108, -0.0322,\n",
       "           0.0176,  0.0527,  0.0941, -0.0132,  0.0397, -0.0193, -0.0001,\n",
       "           0.0213, -0.0204,  0.0206, -0.0676, -0.0797, -0.0829, -0.0929,\n",
       "           0.0083,  0.0649,  0.0093, -0.0020],\n",
       "         [-0.0205,  0.0404,  0.0117, -0.0022,  0.0241, -0.0385, -0.0441,\n",
       "           0.0777,  0.0837, -0.0639, -0.0931, -0.0232, -0.0108, -0.0322,\n",
       "           0.0176,  0.0527,  0.0941, -0.0132,  0.0397, -0.0193, -0.0001,\n",
       "           0.0213, -0.0204,  0.0206, -0.0676, -0.0797, -0.0829, -0.0929,\n",
       "           0.0083,  0.0649,  0.0093, -0.0020],\n",
       "         [-0.0205,  0.0404,  0.0117, -0.0022,  0.0241, -0.0385, -0.0441,\n",
       "           0.0777,  0.0837, -0.0639, -0.0931, -0.0232, -0.0108, -0.0322,\n",
       "           0.0176,  0.0527,  0.0941, -0.0132,  0.0397, -0.0193, -0.0001,\n",
       "           0.0213, -0.0204,  0.0206, -0.0676, -0.0797, -0.0829, -0.0929,\n",
       "           0.0083,  0.0649,  0.0093, -0.0020]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prenorm(Attention(32, 1, 0.0))(torch.zeros(1, 5, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8100d20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, emb_dim=32, hidden_dim=32*3, dropout=0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "ff = FeedForward()\n",
    "ff(torch.ones(1, 5, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4357dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, fn, emb_dim=32, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre_norm = Prenorm(fn, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        res = inp\n",
    "        inp = self.pre_norm(inp)\n",
    "        out = self.dropout(inp) + res\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb14e55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([324, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
    "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
    "    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
    "    omega = 1.0 / (temperature ** omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "posemb_sincos_2d(144 // 8, 144 // 8, 128).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_chans=3, \n",
    "                img_size=IMG_SIZE, \n",
    "                patch_size=PATCH_SIZE, \n",
    "                emb_dim=EMB_DIM, \n",
    "                n_heads=2, \n",
    "                n_layers=4, \n",
    "                dim_out=37,\n",
    "                dropout=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.dim_out = dim_out\n",
    "        self.dropout = dropout\n",
    "        self.n_patches = (img_size ** 2) // (patch_size ** 2) \n",
    "\n",
    "        self.patch_emb = PatchEmbedding(in_chans, patch_size, emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, emb_dim))\n",
    "\n",
    "        self.register_buffer('pos_enc',\n",
    "            posemb_sincos_2d(img_size // patch_size + 1,\n",
    "                            img_size // patch_size + 1,\n",
    "                            emb_dim)\n",
    "            .unsqueeze(0)[:, :self.n_patches+1, :]\n",
    "        )\n",
    "\n",
    "        # self.pos_enc = posemb_sincos_2d(img_size // patch_size + 1, img_size // patch_size + 1, emb_dim) \\\n",
    "        #                                     .unsqueeze(0)[:, :self.n_patches+1, :]\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            transformer_block = nn.Sequential(\n",
    "                TranResBlock(\n",
    "                    fn=Attention(self.emb_dim, self.n_heads, self.dropout),\n",
    "                    emb_dim=emb_dim,\n",
    "                    dropout=dropout\n",
    "                ),\n",
    "                TranResBlock(\n",
    "                    fn=FeedForward(emb_dim=emb_dim, hidden_dim=emb_dim*3, dropout=dropout),\n",
    "                    emb_dim=emb_dim,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "            self.layers.append(transformer_block)\n",
    "\n",
    "        self.fc_cls = nn.Linear(emb_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B = x.shape[0]\n",
    "\n",
    "        patches = self.patch_emb(x)\n",
    "        cls_token = self.cls_token.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        inp = torch.cat([cls_token, patches], dim=1)\n",
    "\n",
    "        inp = inp + self.pos_enc[:, :inp.shape[1], :]\n",
    "\n",
    "        for block in self.layers:\n",
    "            inp = block(inp)\n",
    "\n",
    "        out_logits = self.fc_cls(inp[:, 0, :])\n",
    "        return out_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "Epoch 1, Loss   3.660806\n",
      "Epoch 2, Loss   3.559315\n",
      "Epoch 3, Loss   3.452833\n",
      "Epoch 4, Loss   3.393206\n",
      "Epoch 5, Loss   3.332496\n",
      "Epoch 6, Loss   3.308988\n",
      "Epoch 7, Loss   3.263788\n",
      "Epoch 8, Loss   3.238080\n",
      "Epoch 9, Loss   3.202475\n",
      "Epoch 10, Loss   3.165187\n",
      "Epoch 11, Loss   3.137716\n",
      "Epoch 12, Loss   3.117096\n",
      "Epoch 13, Loss   3.071120\n",
      "Epoch 14, Loss   3.052170\n",
      "Epoch 15, Loss   3.006948\n",
      "Epoch 16, Loss   2.973209\n",
      "Epoch 17, Loss   2.953056\n",
      "Epoch 18, Loss   2.945295\n",
      "Epoch 19, Loss   2.900073\n",
      "Epoch 20, Loss   2.887796\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3392928363.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}, Loss {:10.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./models/vit_v1.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             _save(\n\u001b[1;32m    968\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             super().__init__(\n\u001b[0;32m--> 792\u001b[0;31m                 torch._C.PyTorchFileWriter(\n\u001b[0m\u001b[1;32m    793\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_crc32_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_storage_alignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ./models does not exist."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda'\n",
    "print(\"training on \", device)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, 16, True)\n",
    "\n",
    "vit = ViT(in_chans=3, img_size=144, patch_size=8, emb_dim=32, n_heads=2, n_layers=2, dim_out=37).to(device)\n",
    "\n",
    "optimizer = optim.Adam(vit.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "\n",
    "    vit.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, labels in trainloader:\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = vit(imgs)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    print(\"Epoch {}, Loss {:10.6f}\".format(i, epoch_loss))\n",
    "\n",
    "torch.save(vit.state_dict(), './models/vit_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9934f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 22.64%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "vit.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in trainloader:\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = vit(imgs)\n",
    "        predicted_classes = preds.argmax(dim=1)\n",
    "\n",
    "        correct += (predicted_classes == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Training Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e40d9d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 2.61%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "vit_rand = ViT(in_chans=3, img_size=144, patch_size=8, emb_dim=32, n_heads=2, n_layers=2, dim_out=37).to(device)\n",
    "\n",
    "vit_rand.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in trainloader:\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = vit_rand(imgs)\n",
    "        predicted_classes = preds.argmax(dim=1)\n",
    "\n",
    "        correct += (predicted_classes == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Training Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a8451f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7027027027027026"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guessing randomly\n",
    "(1 / len(dataset.classes)) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
